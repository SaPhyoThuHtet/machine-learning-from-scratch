**3.1 Interpretability**
If a machine learning model performs well, why do we not just trust the model and ignore why it made a certain decision? 
“The problem is that a single metric, such as classification accuracy, is an incomplete description of most real-world tasks.” 
(Doshi-Velez and Kim 2017 6)

**3.2 Taxonomy of Interpretability Methods**

**Intrinsic or post hoc?**
Intrinsic interpretability refers to machine learning models that are considered interpretable due to their simple structure, such as short decision trees or sparse linear models.

Post hoc interpretability refers to the application of interpretation methods after model training. Permutation feature importance is, for example, a post hoc interpretation method.

Post hoc methods can also be applied to intrinsically interpretable models. For example, permutation feature importance can be computed for decision trees.



